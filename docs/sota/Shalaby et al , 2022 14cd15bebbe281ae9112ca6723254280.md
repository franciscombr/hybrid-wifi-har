# Shalaby et al., 2022

Title: Utilizing deep learning models in CSI-based human activity recognition
Year: 2022
Status: Skimmed
Type: Method
Tags: Artificial Intelligence, Convolution neural network, Gated recurrent unit, channel state information, human activity recognition
Last edited: November 29, 2024 11:23 AM
Abstract: In recent years, channel state information (CSI) in WiFi 802.11n has been increasingly used to collect data pertaining to human activity. Such raw data are then used to enhance human activity recognition. Activities such as lying down, falling, walking, running, sitting down, and standing up can now be detected with the use of information collected through CSI. Human activity recognition has a multitude of applications, such as home monitoring of patients. Four deep learning models are presented in this paper, namely: a convolution neural network (CNN) with a Gated Recurrent Unit (GRU); a CNN with a GRU and attention; a CNN with a GRU and a second CNN, and a CNN with Long Short-Term Memory (LSTM) and a second CNN. Those models were trained to perform Human Activity Recognition (HAR) using CSI amplitude data collected by a CSI tool. Experiments conducted to test the efficacy of these models showed superior results compared with other recent approaches. This enhanced performance of our models may be attributable the ability of our models to make full use of available data and to extract all data features, including high dimensionality and time sequence. The highest average recognition accuracy reached by the proposed models was achieved by the CNN-GRU, and the CNN-GRU with attention models, standing at 99.31% and 99.16%, respectively. In addition, the performance of the models was evaluated for unseen CSI data by training our models using a random split-of-dataset method (70% training and 30% testing). Our models achieved impressive results with accuracies reaching 100% for nearly all activities. For the lying down activity, accuracy obtained from the CNN-GRU model stood at 99.46%; slightly higher than the 99.05% achieved by the CNN-GRU with attention model. This confirmed the robustness of our models against environmental changes.
Item Type: Journal Article
Authors: Shalaby, Eman
ElShennawy, Nada
Sarhan, Amany
URL: https://doi.org/10.1007/s00521-021-06787-w
Project: ML

### Overview

- Objective:
    - To develop and compare deep learning models for Human Activity Recognition (HAR) using Channel State Information (CSI) from Wi-Fi signals, emphasizing improved accuracy and robustness for applications such as healthcare and smart homes.
- Key Contributions
    - Proposed and evaluated four novel deep learning models:
        1. CNN-GRU
        2. CNN-GRU with Attention
        3. CNN-GRU-CNN
        4. CNN-LSTM-CNN
    - Achieved high recognition accuracy by integrating CNN for feature extraction and GRU/LSTM for temporal sequence modeling.
    - Demonstrated the superior performance of the CNN-GRU-Attention model with 99.33% accuracy in k-fold validation.
    - Tested models on unseen data using a 70-30 dataset split, achieving real-world robustness.
- Keywords
    - Channel State Information (CSI)
    - Human Activity Recognition (HAR)
    - Convolution Neural Network (CNN)
    - Gated Recurrent Unit (GRU)
    - Attention Mechanism
    - Deep Learning

### Methods

- **Approach/Architecture:**
    - **CNN-GRU:** Uses CNN for spatial feature extraction and GRU for temporal sequence modeling.
    - **CNN-GRU with Attention:** Adds an attention mechanism to focus on relevant temporal features.
    - **CNN-GRU-CNN:** Combines CNN and GRU layers with an additional CNN layer for hierarchical feature learning.
    - **CNN-LSTM-CNN:** Replaces GRU with LSTM in a CNN-GRU-CNN-like architecture for long-term dependency modeling.
- **Dataset:**
    - Name: Publica dataset created for this work
    - Details:
        - **Activities:** Lie Down, Fall, Walk, Run, Sit Down, Stand Up.
        - **Samples:** 8959 samples collected using Intel 5300 NIC in an indoor office environment.
        - **Size:** CSI data with a shape of 1000×30×3 (1000 packets, 30 subcarriers, 3 antennas).
            
            1000×30×31000 \times 30 \times 3
            
        - **Participants:** Six individuals performed each activity 20 times.
    - Preprocessing: CSI amplitude was directly used without noise reduction for preserving signal integrity.
- **Techniques Used:**
    - **Feature Extraction:** CNN layers extract spatial features from CSI data.
    - **Sequence Modeling:**
        - GRU for lightweight and efficient temporal modeling.
        - Attention mechanism for highlighting critical features.
        - LSTM for long-term dependency learning.

### Results

- **Metrics Reported:**
    - Accuracy, Precision, Recall, Area Under Curve (AUC), Loss.
    - Confusion matrices for each model.
- **Performance Highlights:**
    - **CNN-GRU:** Best accuracy on unseen data (99.46%).
    - **CNN-GRU-Attention:** Highest k-fold accuracy (99.33%) and real-world robustness.
    - **CNN-GRU-CNN:** Balanced performance, achieving 98.88% accuracy.
    - **CNN-LSTM-CNN:** Slightly lower accuracy (98.71%) due to higher complexity and longer training times.
- **Comparison to Baselines:**
    - Outperformed LSTM (75% accuracy) and Attention-Based BLSTM (95% accuracy) models from prior studies.
    - CNN-GRU-Attention model demonstrated the best trade-off between accuracy, training time, and computational complexity.

### Analysis

- **Strengths:**
    - Integration of CNN and GRU/LSTM effectively combines spatial and temporal features.
    - Attention mechanism enhances interpretability and robustness to environmental variations.
    - High accuracy achieved for real-world data with efficient training and testing times.
- **Weaknesses/Gaps:**
    - Dataset is limited to six activities performed in a controlled indoor environment, which may limit generalization.
    - The study does not explore multi-user scenarios or environments with dynamic noise.
- **Opportunities for Improvement:**
    - Extend experiments to datasets with phase information or additional activities.
    - Incorporate denoising techniques to address noisy CSI environments.
    - Explore lightweight models for real-time edge deployment.

### Relevance to the Project:

- **Insights or Ideas:**
    - The hybrid CNN-GRU and CNN-GRU-Attention architectures provide a strong baseline for integrating Vision Transformers in your work.
    - The attention mechanism used here can inspire feature weighting approaches in transformer-based models.
- **Potential for Reuse:**
    - The CNN-GRU-Attention model structure and methodology can serve as a comparison baseline for evaluating your Vision Transformer approach.
    - Dataset and preprocessing techniques can be directly utilized or adapted for benchmarking.
- **Unanswered Questions:**
    - How do the models perform on datasets with additional features like phase or multi-environment data?
    - What is the scalability of these models for edge computing in IoT applications?